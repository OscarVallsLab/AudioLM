{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n337KoD2om3L",
    "outputId": "97ada0c6-f21c-483e-d63d-08abddd49004"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLa ejecuciÃ³n de celdas con 'base' requiere el paquete ipykernel.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n base ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "\n",
    "# If this doesn't work, there's no GPU available or detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TLJAcUHpvmp4",
    "outputId": "95bcda95-a484-40c6-e5a7-47f4378759a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: audiolm-pytorch in ./env/lib/python3.10/site-packages (1.8.2)\n",
      "Requirement already satisfied: accelerate>=0.24.0 in ./env/lib/python3.10/site-packages (from audiolm-pytorch) (0.24.1)\n",
      "Requirement already satisfied: beartype>=0.16.1 in ./env/lib/python3.10/site-packages (from audiolm-pytorch) (0.16.4)\n",
      "Requirement already satisfied: einops>=0.7.0 in ./env/lib/python3.10/site-packages (from audiolm-pytorch) (0.7.0)\n",
      "Requirement already satisfied: ema-pytorch>=0.2.2 in ./env/lib/python3.10/site-packages (from audiolm-pytorch) (0.3.1)\n",
      "Requirement already satisfied: encodec in ./env/lib/python3.10/site-packages (from audiolm-pytorch) (0.1.1)\n",
      "Requirement already satisfied: fairseq in ./env/lib/python3.10/site-packages (from audiolm-pytorch) (0.12.2)\n",
      "Requirement already satisfied: gateloop-transformer>=0.0.24 in ./env/lib/python3.10/site-packages (from audiolm-pytorch) (0.0.26)\n",
      "Requirement already satisfied: joblib in ./env/lib/python3.10/site-packages (from audiolm-pytorch) (1.3.2)\n",
      "Requirement already satisfied: local-attention>=1.9.0 in ./env/lib/python3.10/site-packages (from audiolm-pytorch) (1.9.0)\n",
      "Requirement already satisfied: scikit-learn in ./env/lib/python3.10/site-packages (from audiolm-pytorch) (1.3.2)\n",
      "Requirement already satisfied: sentencepiece in ./env/lib/python3.10/site-packages (from audiolm-pytorch) (0.1.99)\n",
      "Requirement already satisfied: torch>=1.12 in ./env/lib/python3.10/site-packages (from audiolm-pytorch) (2.1.1)\n",
      "Requirement already satisfied: torchaudio in ./env/lib/python3.10/site-packages (from audiolm-pytorch) (2.1.1)\n",
      "Requirement already satisfied: transformers in ./env/lib/python3.10/site-packages (from audiolm-pytorch) (4.35.2)\n",
      "Requirement already satisfied: tqdm in ./env/lib/python3.10/site-packages (from audiolm-pytorch) (4.66.1)\n",
      "Requirement already satisfied: vector-quantize-pytorch>=1.11.3 in ./env/lib/python3.10/site-packages (from audiolm-pytorch) (1.11.7)\n",
      "Requirement already satisfied: numpy>=1.17 in ./env/lib/python3.10/site-packages (from accelerate>=0.24.0->audiolm-pytorch) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in ./env/lib/python3.10/site-packages (from accelerate>=0.24.0->audiolm-pytorch) (23.2)\n",
      "Requirement already satisfied: psutil in ./env/lib/python3.10/site-packages (from accelerate>=0.24.0->audiolm-pytorch) (5.9.6)\n",
      "Requirement already satisfied: pyyaml in ./env/lib/python3.10/site-packages (from accelerate>=0.24.0->audiolm-pytorch) (6.0.1)\n",
      "Requirement already satisfied: huggingface-hub in ./env/lib/python3.10/site-packages (from accelerate>=0.24.0->audiolm-pytorch) (0.19.4)\n",
      "Requirement already satisfied: rotary-embedding-torch in ./env/lib/python3.10/site-packages (from gateloop-transformer>=0.0.24->audiolm-pytorch) (0.3.5)\n",
      "Requirement already satisfied: filelock in ./env/lib/python3.10/site-packages (from torch>=1.12->audiolm-pytorch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in ./env/lib/python3.10/site-packages (from torch>=1.12->audiolm-pytorch) (4.8.0)\n",
      "Requirement already satisfied: sympy in ./env/lib/python3.10/site-packages (from torch>=1.12->audiolm-pytorch) (1.12)\n",
      "Requirement already satisfied: networkx in ./env/lib/python3.10/site-packages (from torch>=1.12->audiolm-pytorch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in ./env/lib/python3.10/site-packages (from torch>=1.12->audiolm-pytorch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in ./env/lib/python3.10/site-packages (from torch>=1.12->audiolm-pytorch) (2023.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./env/lib/python3.10/site-packages (from torch>=1.12->audiolm-pytorch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./env/lib/python3.10/site-packages (from torch>=1.12->audiolm-pytorch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./env/lib/python3.10/site-packages (from torch>=1.12->audiolm-pytorch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in ./env/lib/python3.10/site-packages (from torch>=1.12->audiolm-pytorch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./env/lib/python3.10/site-packages (from torch>=1.12->audiolm-pytorch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./env/lib/python3.10/site-packages (from torch>=1.12->audiolm-pytorch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./env/lib/python3.10/site-packages (from torch>=1.12->audiolm-pytorch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./env/lib/python3.10/site-packages (from torch>=1.12->audiolm-pytorch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./env/lib/python3.10/site-packages (from torch>=1.12->audiolm-pytorch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in ./env/lib/python3.10/site-packages (from torch>=1.12->audiolm-pytorch) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./env/lib/python3.10/site-packages (from torch>=1.12->audiolm-pytorch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in ./env/lib/python3.10/site-packages (from torch>=1.12->audiolm-pytorch) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./env/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.12->audiolm-pytorch) (12.3.101)\n",
      "Requirement already satisfied: cffi in ./env/lib/python3.10/site-packages (from fairseq->audiolm-pytorch) (1.16.0)\n",
      "Requirement already satisfied: cython in ./env/lib/python3.10/site-packages (from fairseq->audiolm-pytorch) (3.0.5)\n",
      "Requirement already satisfied: hydra-core<1.1,>=1.0.7 in ./env/lib/python3.10/site-packages (from fairseq->audiolm-pytorch) (1.0.7)\n",
      "Requirement already satisfied: omegaconf<2.1 in ./env/lib/python3.10/site-packages (from fairseq->audiolm-pytorch) (2.0.6)\n",
      "Requirement already satisfied: regex in ./env/lib/python3.10/site-packages (from fairseq->audiolm-pytorch) (2023.10.3)\n",
      "Requirement already satisfied: sacrebleu>=1.4.12 in ./env/lib/python3.10/site-packages (from fairseq->audiolm-pytorch) (2.3.2)\n",
      "Requirement already satisfied: bitarray in ./env/lib/python3.10/site-packages (from fairseq->audiolm-pytorch) (2.8.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in ./env/lib/python3.10/site-packages (from scikit-learn->audiolm-pytorch) (1.11.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./env/lib/python3.10/site-packages (from scikit-learn->audiolm-pytorch) (3.2.0)\n",
      "Requirement already satisfied: requests in ./env/lib/python3.10/site-packages (from transformers->audiolm-pytorch) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in ./env/lib/python3.10/site-packages (from transformers->audiolm-pytorch) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in ./env/lib/python3.10/site-packages (from transformers->audiolm-pytorch) (0.4.0)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.8 in ./env/lib/python3.10/site-packages (from hydra-core<1.1,>=1.0.7->fairseq->audiolm-pytorch) (4.8)\n",
      "Requirement already satisfied: portalocker in ./env/lib/python3.10/site-packages (from sacrebleu>=1.4.12->fairseq->audiolm-pytorch) (2.8.2)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in ./env/lib/python3.10/site-packages (from sacrebleu>=1.4.12->fairseq->audiolm-pytorch) (0.9.0)\n",
      "Requirement already satisfied: colorama in ./env/lib/python3.10/site-packages (from sacrebleu>=1.4.12->fairseq->audiolm-pytorch) (0.4.6)\n",
      "Requirement already satisfied: lxml in ./env/lib/python3.10/site-packages (from sacrebleu>=1.4.12->fairseq->audiolm-pytorch) (4.9.3)\n",
      "Requirement already satisfied: pycparser in ./env/lib/python3.10/site-packages (from cffi->fairseq->audiolm-pytorch) (2.21)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./env/lib/python3.10/site-packages (from jinja2->torch>=1.12->audiolm-pytorch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./env/lib/python3.10/site-packages (from requests->transformers->audiolm-pytorch) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./env/lib/python3.10/site-packages (from requests->transformers->audiolm-pytorch) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./env/lib/python3.10/site-packages (from requests->transformers->audiolm-pytorch) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./env/lib/python3.10/site-packages (from requests->transformers->audiolm-pytorch) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./env/lib/python3.10/site-packages (from sympy->torch>=1.12->audiolm-pytorch) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: tensorboardX in ./env/lib/python3.10/site-packages (2.6.2.2)\n",
      "Requirement already satisfied: numpy in ./env/lib/python3.10/site-packages (from tensorboardX) (1.26.2)\n",
      "Requirement already satisfied: packaging in ./env/lib/python3.10/site-packages (from tensorboardX) (23.2)\n",
      "Requirement already satisfied: protobuf>=3.20 in ./env/lib/python3.10/site-packages (from tensorboardX) (4.25.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install audiolm-pytorch\n",
    "!pip install tensorboardX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xuNcsDJsvQwh"
   },
   "source": [
    "## Setup\n",
    "\n",
    "Includes:\n",
    "\n",
    "- How to generate a placeholder dataset if you haven't already, just the basics to run \"training\" e2e on a tiny dataset\n",
    "- How to download a dataset from OpenSLR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jBxNK5cKW--_"
   },
   "source": [
    "### Imports & paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "OrNeKngVVM0L"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oscar/dipsy_speech/AudioLM/env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import math\n",
    "import wave\n",
    "import struct\n",
    "import os\n",
    "import urllib.request\n",
    "import tarfile\n",
    "from audiolm_pytorch import SoundStream, SoundStreamTrainer, HubertWithKmeans, SemanticTransformer, SemanticTransformerTrainer, HubertWithKmeans, CoarseTransformer, CoarseTransformerWrapper, CoarseTransformerTrainer, FineTransformer, FineTransformerWrapper, FineTransformerTrainer, AudioLM\n",
    "from torch import nn\n",
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "\n",
    "# define all dataset paths, checkpoints, etc\n",
    "dataset_folder = \"placeholder_dataset\"\n",
    "soundstream_ckpt = \"results/soundstream.8.pt\" # this can change depending on number of steps\n",
    "hubert_ckpt = 'hubert/hubert_base_ls960.pt'\n",
    "hubert_quantizer = f'hubert/hubert_base_ls960_L9_km500.bin' # listed in row \"HuBERT Base (~95M params)\", column Quantizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pA56YODZXBtf"
   },
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "6nnPceFWwedh"
   },
   "outputs": [],
   "source": [
    "# Placeholder data generation\n",
    "def get_sinewave(freq=440.0, duration_ms=200, volume=1.0, sample_rate=44100.0):\n",
    "  # code adapted from https://stackoverflow.com/a/33913403\n",
    "  audio = []\n",
    "  num_samples = duration_ms * (sample_rate / 1000.0)\n",
    "  for x in range(int(num_samples)):\n",
    "    audio.append(volume * math.sin(2 * math.pi * freq * (x / sample_rate)))\n",
    "  return audio\n",
    "\n",
    "def save_wav(file_name, audio, sample_rate=44100.0):\n",
    "  # Open up a wav file\n",
    "  wav_file=wave.open(file_name,\"w\")\n",
    "  # wav params\n",
    "  nchannels = 1\n",
    "  sampwidth = 2\n",
    "  # 44100 is the industry standard sample rate - CD quality.  If you need to\n",
    "  # save on file size you can adjust it downwards. The stanard for low quality\n",
    "  # is 8000 or 8kHz.\n",
    "  nframes = len(audio)\n",
    "  comptype = \"NONE\"\n",
    "  compname = \"not compressed\"\n",
    "  wav_file.setparams((nchannels, sampwidth, sample_rate, nframes, comptype, compname))\n",
    "  # WAV files here are using short, 16 bit, signed integers for the \n",
    "  # sample size.  So we multiply the floating point data we have by 32767, the\n",
    "  # maximum value for a short integer.  NOTE: It is theortically possible to\n",
    "  # use the floating point -1.0 to 1.0 data directly in a WAV file but not\n",
    "  # obvious how to do that using the wave module in python.\n",
    "  for sample in audio:\n",
    "      wav_file.writeframes(struct.pack('h', int( sample * 32767.0 )))\n",
    "  wav_file.close()\n",
    "  return\n",
    "\n",
    "def make_placeholder_dataset():\n",
    "  # Make a placeholder dataset with a few .wav files that you can \"train\" on, just to verify things work e2e\n",
    "  if os.path.isdir(dataset_folder):\n",
    "    return\n",
    "  os.makedirs(dataset_folder)\n",
    "  save_wav(f\"{dataset_folder}/example.wav\", get_sinewave())\n",
    "  save_wav(f\"{dataset_folder}/example2.wav\", get_sinewave(duration_ms=500))\n",
    "  os.makedirs(f\"{dataset_folder}/subdirectory\")\n",
    "  save_wav(f\"{dataset_folder}/subdirectory/example.wav\", get_sinewave(freq=330.0))\n",
    "\n",
    "make_placeholder_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "jwYCbFpHvmRI"
   },
   "outputs": [],
   "source": [
    "# Get actual dataset. Uncomment this if you want to try training on real data\n",
    "\n",
    "# full dataset: https://www.openslr.org/12\n",
    "# We'll use https://us.openslr.org/resources/12/dev-clean.tar.gz development set, \"clean\" speech.\n",
    "# We *should* train on, well, training, but this is just to demo running things end-to-end at all so I just picked a small clean set.\n",
    "\n",
    "# url = \"https://us.openslr.org/resources/12/dev-clean.tar.gz\"\n",
    "# filename = \"dev-clean\"\n",
    "# filename_targz = filename + \".tar.gz\"\n",
    "# if not os.path.isfile(filename_targz):\n",
    "#   urllib.request.urlretrieve(url, filename_targz)\n",
    "# if not os.path.isdir(filename):\n",
    "#   # open file\n",
    "#   with tarfile.open(filename_targz) as t:\n",
    "#     t.extractall(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PYcI0aXEwuxR"
   },
   "source": [
    "## Training\n",
    "\n",
    "Now that we have a dataset, we can train AudioLM.\n",
    "\n",
    "**Note**: do NOT type \"y\" to overwrite previous experiments/ checkpoints when running through the cells here unless you're ready to the entire results folder! Otherwise you will end up erasing things (e.g. you train SoundStream first, and if you choose \"overwrite\" then you lose the SoundStream checkpoint when you then train SemanticTransformer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T7GiyBcBWiZV"
   },
   "source": [
    "### SoundStream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nGU0OZiOwPEO",
    "outputId": "21dd959c-6458-4477-8403-cf810166f38d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training with dataset of 2 samples and validating with randomly splitted 1 samples\n",
      "0: soundstream total loss: 31.025, soundstream recon loss: 0.160 | discr (scale 1) loss: 2.000 | discr (scale 0.5) loss: 2.000 | discr (scale 0.25) loss: 2.000\n",
      "0: saving to results\n",
      "0: saving model to results\n",
      "1: soundstream total loss: 31.475, soundstream recon loss: 0.158 | discr (scale 1) loss: 1.996 | discr (scale 0.5) loss: 1.996 | discr (scale 0.25) loss: 1.997\n",
      "2: soundstream total loss: 33.001, soundstream recon loss: 0.157 | discr (scale 1) loss: 1.991 | discr (scale 0.5) loss: 1.990 | discr (scale 0.25) loss: 1.993\n",
      "2: saving to results\n",
      "3: soundstream total loss: 32.160, soundstream recon loss: 0.156 | discr (scale 1) loss: 1.982 | discr (scale 0.5) loss: 1.982 | discr (scale 0.25) loss: 1.987\n",
      "4: soundstream total loss: 33.274, soundstream recon loss: 0.158 | discr (scale 1) loss: 1.971 | discr (scale 0.5) loss: 1.971 | discr (scale 0.25) loss: 1.979\n",
      "4: saving to results\n",
      "4: saving model to results\n",
      "5: soundstream total loss: 32.258, soundstream recon loss: 0.158 | discr (scale 1) loss: 1.957 | discr (scale 0.5) loss: 1.958 | discr (scale 0.25) loss: 1.969\n",
      "6: soundstream total loss: 32.984, soundstream recon loss: 0.158 | discr (scale 1) loss: 1.939 | discr (scale 0.5) loss: 1.940 | discr (scale 0.25) loss: 1.957\n",
      "6: saving to results\n",
      "7: soundstream total loss: 31.649, soundstream recon loss: 0.157 | discr (scale 1) loss: 1.916 | discr (scale 0.5) loss: 1.919 | discr (scale 0.25) loss: 1.940\n",
      "8: soundstream total loss: 29.799, soundstream recon loss: 0.156 | discr (scale 1) loss: 1.887 | discr (scale 0.5) loss: 1.896 | discr (scale 0.25) loss: 1.920\n",
      "8: saving to results\n",
      "8: saving model to results\n",
      "training complete\n"
     ]
    }
   ],
   "source": [
    "soundstream = SoundStream(\n",
    "    codebook_size = 1024,\n",
    "    rq_num_quantizers = 8,\n",
    ")\n",
    "\n",
    "trainer = SoundStreamTrainer(\n",
    "    soundstream,\n",
    "    folder = dataset_folder,\n",
    "    batch_size = 1,\n",
    "    grad_accum_every = 8,         # effective batch size of 32\n",
    "    data_max_length = 320 * 32,\n",
    "    save_results_every = 2,\n",
    "    save_model_every = 4,\n",
    "    num_train_steps = 9\n",
    ").cuda()\n",
    "# NOTE: I changed num_train_steps to 9 (aka 8 + 1) from 10000 to make things go faster for demo purposes\n",
    "# adjusting save_*_every variables for the same reason\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lqjN28L4Wc5Q"
   },
   "source": [
    "### SemanticTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qgd962eSvDzS",
    "outputId": "b0550cde-0c8b-4a39-f896-f6f813f50f8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training with dataset of 2 samples and validating with randomly splitted 1 samples\n",
      "0: loss: 6.585640907287598\n",
      "0: valid loss 5.270576000213623\n",
      "0: saving model to results\n",
      "training complete\n"
     ]
    }
   ],
   "source": [
    "# hubert checkpoints can be downloaded at\n",
    "# https://github.com/facebookresearch/fairseq/tree/main/examples/hubert\n",
    "if not os.path.isdir(\"hubert\"):\n",
    "  os.makedirs(\"hubert\")\n",
    "if not os.path.isfile(hubert_ckpt):\n",
    "  hubert_ckpt_download = f\"https://dl.fbaipublicfiles.com/{hubert_ckpt}\"\n",
    "  urllib.request.urlretrieve(hubert_ckpt_download, f\"./{hubert_ckpt}\")\n",
    "if not os.path.isfile(hubert_quantizer):\n",
    "  hubert_quantizer_download = f\"https://dl.fbaipublicfiles.com/{hubert_quantizer}\"\n",
    "  urllib.request.urlretrieve(hubert_quantizer_download, f\"./{hubert_quantizer}\")\n",
    "\n",
    "wav2vec = HubertWithKmeans(\n",
    "    checkpoint_path = f'./{hubert_ckpt}',\n",
    "    kmeans_path = f'./{hubert_quantizer}'\n",
    ")\n",
    "\n",
    "semantic_transformer = SemanticTransformer(\n",
    "    num_semantic_tokens = wav2vec.codebook_size,\n",
    "    dim = 1024,\n",
    "    depth = 6\n",
    ").cuda()\n",
    "\n",
    "\n",
    "trainer = SemanticTransformerTrainer(\n",
    "    transformer = semantic_transformer,\n",
    "    wav2vec = wav2vec,\n",
    "    folder = dataset_folder,\n",
    "    batch_size = 1,\n",
    "    data_max_length = 320 * 32,\n",
    "    num_train_steps = 1\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4eEvIzhEWwRz"
   },
   "source": [
    "### CoarseTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1LeWmaNHzzY9",
    "outputId": "7e7ecb3b-f59e-4d18-c8c9-64762e9b43fc"
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/oscar/dipsy_speech/AudioLM/audiolm_pytorch_demo.ipynb Celda 15\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/oscar/dipsy_speech/AudioLM/audiolm_pytorch_demo.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m wav2vec \u001b[39m=\u001b[39m HubertWithKmeans(\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/oscar/dipsy_speech/AudioLM/audiolm_pytorch_demo.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     checkpoint_path \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m./\u001b[39m\u001b[39m{\u001b[39;00mhubert_ckpt\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/oscar/dipsy_speech/AudioLM/audiolm_pytorch_demo.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     kmeans_path \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m./\u001b[39m\u001b[39m{\u001b[39;00mhubert_quantizer\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/oscar/dipsy_speech/AudioLM/audiolm_pytorch_demo.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m )\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/oscar/dipsy_speech/AudioLM/audiolm_pytorch_demo.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m soundstream \u001b[39m=\u001b[39m SoundStream(\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/oscar/dipsy_speech/AudioLM/audiolm_pytorch_demo.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     codebook_size \u001b[39m=\u001b[39m \u001b[39m1024\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/oscar/dipsy_speech/AudioLM/audiolm_pytorch_demo.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     rq_num_quantizers \u001b[39m=\u001b[39m \u001b[39m8\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/oscar/dipsy_speech/AudioLM/audiolm_pytorch_demo.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m )\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/oscar/dipsy_speech/AudioLM/audiolm_pytorch_demo.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m soundstream\u001b[39m.\u001b[39;49mload(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m./\u001b[39;49m\u001b[39m{\u001b[39;49;00msoundstream_ckpt\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/oscar/dipsy_speech/AudioLM/audiolm_pytorch_demo.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m coarse_transformer \u001b[39m=\u001b[39m CoarseTransformer(\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/oscar/dipsy_speech/AudioLM/audiolm_pytorch_demo.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     num_semantic_tokens \u001b[39m=\u001b[39m wav2vec\u001b[39m.\u001b[39mcodebook_size,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/oscar/dipsy_speech/AudioLM/audiolm_pytorch_demo.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     codebook_size \u001b[39m=\u001b[39m \u001b[39m1024\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/oscar/dipsy_speech/AudioLM/audiolm_pytorch_demo.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m     depth \u001b[39m=\u001b[39m \u001b[39m6\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/oscar/dipsy_speech/AudioLM/audiolm_pytorch_demo.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/oscar/dipsy_speech/AudioLM/audiolm_pytorch_demo.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m trainer \u001b[39m=\u001b[39m CoarseTransformerTrainer(\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/oscar/dipsy_speech/AudioLM/audiolm_pytorch_demo.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m     transformer \u001b[39m=\u001b[39m coarse_transformer,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/oscar/dipsy_speech/AudioLM/audiolm_pytorch_demo.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m     codec \u001b[39m=\u001b[39m soundstream,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/oscar/dipsy_speech/AudioLM/audiolm_pytorch_demo.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m     num_train_steps \u001b[39m=\u001b[39m \u001b[39m9\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/oscar/dipsy_speech/AudioLM/audiolm_pytorch_demo.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m )\n",
      "File \u001b[0;32m~/dipsy_speech/AudioLM/audiolm_pytorch/soundstream.py:732\u001b[0m, in \u001b[0;36mSoundStream.load\u001b[0;34m(self, path, strict)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(\u001b[39mself\u001b[39m, path, strict \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m    731\u001b[0m     path \u001b[39m=\u001b[39m Path(path)\n\u001b[0;32m--> 732\u001b[0m     \u001b[39massert\u001b[39;00m path\u001b[39m.\u001b[39mexists()\n\u001b[1;32m    733\u001b[0m     pkg \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mload(\u001b[39mstr\u001b[39m(path), map_location \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    735\u001b[0m     \u001b[39m# check version\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "wav2vec = HubertWithKmeans(\n",
    "    checkpoint_path = f'./{hubert_ckpt}',\n",
    "    kmeans_path = f'./{hubert_quantizer}'\n",
    ")\n",
    "\n",
    "soundstream = SoundStream(\n",
    "    codebook_size = 1024,\n",
    "    rq_num_quantizers = 8,\n",
    ")\n",
    "\n",
    "soundstream.load(f\"./{soundstream_ckpt}\")\n",
    "\n",
    "coarse_transformer = CoarseTransformer(\n",
    "    num_semantic_tokens = wav2vec.codebook_size,\n",
    "    codebook_size = 1024,\n",
    "    num_coarse_quantizers = 3,\n",
    "    dim = 512,\n",
    "    depth = 6\n",
    ")\n",
    "\n",
    "trainer = CoarseTransformerTrainer(\n",
    "    transformer = coarse_transformer,\n",
    "    codec = soundstream,\n",
    "    wav2vec = wav2vec,\n",
    "    folder = dataset_folder,\n",
    "    batch_size = 1,\n",
    "    data_max_length = 320 * 32,\n",
    "    save_results_every = 2,\n",
    "    save_model_every = 4,\n",
    "    num_train_steps = 9\n",
    ")\n",
    "# NOTE: I changed num_train_steps to 9 (aka 8 + 1) from 10000 to make things go faster for demo purposes\n",
    "# adjusting save_*_every variables for the same reason\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fRvj7qOJWzmw"
   },
   "source": [
    "### FineTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZRaEhRRKWg8F",
    "outputId": "7cc166c4-c8e9-45ef-8293-8f5381c2d3af"
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/oscar/dipsy_speech/AudioLM/audiolm_pytorch_demo.ipynb Celda 17\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/oscar/dipsy_speech/AudioLM/audiolm_pytorch_demo.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m soundstream \u001b[39m=\u001b[39m SoundStream(\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/oscar/dipsy_speech/AudioLM/audiolm_pytorch_demo.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     codebook_size \u001b[39m=\u001b[39m \u001b[39m1024\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/oscar/dipsy_speech/AudioLM/audiolm_pytorch_demo.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     rq_num_quantizers \u001b[39m=\u001b[39m \u001b[39m8\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/oscar/dipsy_speech/AudioLM/audiolm_pytorch_demo.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m )\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/oscar/dipsy_speech/AudioLM/audiolm_pytorch_demo.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m soundstream\u001b[39m.\u001b[39;49mload(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m./\u001b[39;49m\u001b[39m{\u001b[39;49;00msoundstream_ckpt\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/oscar/dipsy_speech/AudioLM/audiolm_pytorch_demo.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m fine_transformer \u001b[39m=\u001b[39m FineTransformer(\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/oscar/dipsy_speech/AudioLM/audiolm_pytorch_demo.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     num_coarse_quantizers \u001b[39m=\u001b[39m \u001b[39m3\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/oscar/dipsy_speech/AudioLM/audiolm_pytorch_demo.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     num_fine_quantizers \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/oscar/dipsy_speech/AudioLM/audiolm_pytorch_demo.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     depth \u001b[39m=\u001b[39m \u001b[39m6\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/oscar/dipsy_speech/AudioLM/audiolm_pytorch_demo.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/oscar/dipsy_speech/AudioLM/audiolm_pytorch_demo.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m trainer \u001b[39m=\u001b[39m FineTransformerTrainer(\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/oscar/dipsy_speech/AudioLM/audiolm_pytorch_demo.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m     transformer \u001b[39m=\u001b[39m fine_transformer,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/oscar/dipsy_speech/AudioLM/audiolm_pytorch_demo.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m     codec \u001b[39m=\u001b[39m soundstream,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/oscar/dipsy_speech/AudioLM/audiolm_pytorch_demo.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m     num_train_steps \u001b[39m=\u001b[39m \u001b[39m9\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/oscar/dipsy_speech/AudioLM/audiolm_pytorch_demo.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m )\n",
      "File \u001b[0;32m~/dipsy_speech/AudioLM/audiolm_pytorch/soundstream.py:732\u001b[0m, in \u001b[0;36mSoundStream.load\u001b[0;34m(self, path, strict)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(\u001b[39mself\u001b[39m, path, strict \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m    731\u001b[0m     path \u001b[39m=\u001b[39m Path(path)\n\u001b[0;32m--> 732\u001b[0m     \u001b[39massert\u001b[39;00m path\u001b[39m.\u001b[39mexists()\n\u001b[1;32m    733\u001b[0m     pkg \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mload(\u001b[39mstr\u001b[39m(path), map_location \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    735\u001b[0m     \u001b[39m# check version\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "soundstream = SoundStream(\n",
    "    codebook_size = 1024,\n",
    "    rq_num_quantizers = 8,\n",
    ")\n",
    "\n",
    "soundstream.load(f\"./{soundstream_ckpt}\")\n",
    "\n",
    "fine_transformer = FineTransformer(\n",
    "    num_coarse_quantizers = 3,\n",
    "    num_fine_quantizers = 5,\n",
    "    codebook_size = 1024,\n",
    "    dim = 512,\n",
    "    depth = 6\n",
    ")\n",
    "\n",
    "trainer = FineTransformerTrainer(\n",
    "    transformer = fine_transformer,\n",
    "    codec = soundstream,\n",
    "    folder = dataset_folder,\n",
    "    batch_size = 1,\n",
    "    data_max_length = 320 * 32,\n",
    "    num_train_steps = 9\n",
    ")\n",
    "# NOTE: I changed num_train_steps to 9 (aka 8 + 1) from 10000 to make things go faster for demo purposes\n",
    "# adjusting save_*_every variables for the same reason\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QoHgkgA3XKXH"
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rzghrux5WinW",
    "outputId": "9dd39f7f-0046-4a5f-826e-a442345987af"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'coarse_transformer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/oscar/dipsy_speech/AudioLM/audiolm_pytorch_demo.ipynb Celda 19\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/oscar/dipsy_speech/AudioLM/audiolm_pytorch_demo.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Everything together\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/oscar/dipsy_speech/AudioLM/audiolm_pytorch_demo.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m audiolm \u001b[39m=\u001b[39m AudioLM(\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/oscar/dipsy_speech/AudioLM/audiolm_pytorch_demo.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     wav2vec \u001b[39m=\u001b[39m wav2vec,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/oscar/dipsy_speech/AudioLM/audiolm_pytorch_demo.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     codec \u001b[39m=\u001b[39m soundstream,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/oscar/dipsy_speech/AudioLM/audiolm_pytorch_demo.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     semantic_transformer \u001b[39m=\u001b[39m semantic_transformer,\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/oscar/dipsy_speech/AudioLM/audiolm_pytorch_demo.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     coarse_transformer \u001b[39m=\u001b[39m coarse_transformer,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/oscar/dipsy_speech/AudioLM/audiolm_pytorch_demo.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     fine_transformer \u001b[39m=\u001b[39m fine_transformer\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/oscar/dipsy_speech/AudioLM/audiolm_pytorch_demo.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/oscar/dipsy_speech/AudioLM/audiolm_pytorch_demo.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m generated_wav \u001b[39m=\u001b[39m audiolm(batch_size \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'coarse_transformer' is not defined"
     ]
    }
   ],
   "source": [
    "# Everything together\n",
    "audiolm = AudioLM(\n",
    "    wav2vec = wav2vec,\n",
    "    codec = soundstream,\n",
    "    semantic_transformer = semantic_transformer,\n",
    "    coarse_transformer = coarse_transformer,\n",
    "    fine_transformer = fine_transformer\n",
    ")\n",
    "\n",
    "generated_wav = audiolm(batch_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "4rQPHTSRngEr"
   },
   "outputs": [],
   "source": [
    "output_path = \"out.wav\"\n",
    "sample_rate = 44100\n",
    "torchaudio.save(output_path, generated_wav.cpu(), sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "is9wLY_ncDYK"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
